# Model Analysis

This folder contains the evaluation pipeline for comparing the **raw LLaMA-3.1 model** against the **fine-tuned hybrid model** on the support ticket prediction task.  
The goal is to measure performance across **Urgency Code**, **Category**, and **Resolution** prediction.

---

## üìÇ Contents

- `raw_Lama3.py` ‚Üí Runs inference with the raw, untrained **LLaMA-3.1-8B-Instruct** model on the test set.  
- `hybrid_test.py` ‚Üí Runs inference with the **fine-tuned hybrid model** on the same test set.  
- `raw_results.csv` ‚Üí Predictions (urgency, category, resolution) generated by the raw model.  
- `hybrid_results.csv` ‚Üí Predictions generated by the fine-tuned hybrid model.  
- `evaluate_models_outputs.py` ‚Üí Enriches the model outputs with additional evaluation signals, including:
  - Fuzzy matching scores for resolution texts  
  - Sentence-BERT embedding similarities  
  - Retriever similarity features  
- `model_analysis.ipynb` ‚Üí Jupyter notebook that **compares results from both models**, producing:
  - Accuracy of urgency and category predictions  
  - Fuzzy matching averages for resolutions  
  - Embedding similarity distributions  
  - Side-by-side performance metrics  

---

## ‚öôÔ∏è Workflow

1. **Run inference**  
   - `raw_Lama3.py` ‚Üí generate baseline predictions  
   - `hybrid_test.py` ‚Üí generate fine-tuned model predictions  

2. **Evaluate outputs**  
   - Use `evaluate_models_outputs.py` to process predictions into enriched result files (`raw_results.csv`, `hybrid_results.csv`).  

3. **Compare models**  
   - Open `model_analysis.ipynb` to compare performance between raw and fine-tuned models across multiple metrics.  

---

## üìä Purpose

This analysis highlights the improvements achieved through fine-tuning:
- More accurate **Urgency Code** and **Category** predictions.  
- Higher **resolution similarity** to ground truth (via fuzzy matching and embeddings).  
- Overall stronger alignment between predicted resolutions and true support ticket solutions.  

